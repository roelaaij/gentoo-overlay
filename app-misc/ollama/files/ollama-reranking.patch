From 1071879d77128c7b0cf7b14ee213b8436f15457a Mon Sep 17 00:00:00 2001
From: Liu Yuan <namei.unix@gmail.com>
Date: Tue, 29 Oct 2024 22:17:44 +0800
Subject: [PATCH 1/5] go runner: add rerank support

Co-authored-by Craig Hughes
Signed-off-by: Liu Yuan <namei.unix@gmail.com>
---
 llama/llama.go               | 22 ++++++++-
 runner/llamarunner/runner.go | 95 +++++++++++++++++++++++++++++++++++-
 2 files changed, 113 insertions(+), 4 deletions(-)

diff --git a/llama/llama.go b/llama/llama.go
index 0c4fca43..73b9452a 100644
--- a/llama/llama.go
+++ b/llama/llama.go
@@ -111,7 +111,7 @@ type ContextParams struct {
 	c C.struct_llama_context_params
 }
 
-func NewContextParams(numCtx int, batchSize int, numSeqMax int, threads int, flashAttention bool, kvCacheType string) ContextParams {
+func NewContextParams(numCtx int, batchSize int, numSeqMax int, threads int, flashAttention bool, kvCacheType string, reranking bool) ContextParams {
 	params := C.llama_context_default_params()
 	params.n_ctx = C.uint(numCtx)
 	params.n_batch = C.uint(batchSize)
@@ -122,7 +122,9 @@ func NewContextParams(numCtx int, batchSize int, numSeqMax int, threads int, fla
 	params.flash_attn = C.bool(flashAttention)
 	params.type_k = kvCacheTypeFromStr(strings.ToLower(kvCacheType))
 	params.type_v = kvCacheTypeFromStr(strings.ToLower(kvCacheType))
-
+	if reranking {
+		params.pooling_type = C.LLAMA_POOLING_TYPE_RANK
+	}
 	return ContextParams{c: params}
 }
 
@@ -214,6 +216,18 @@ func (c *Context) GetEmbeddingsIth(i int) []float32 {
 	return embeddings
 }
 
+func (c *Context) GetTokenBOS() C.llama_token {
+	return C.llama_token_bos(c.Model().c)
+}
+
+func (c *Context) GetTokenEOS() C.llama_token {
+	return C.llama_token_eos(c.Model().c)
+}
+
+func (c *Context) GetTokenSEP() C.llama_token {
+	return C.llama_token_sep(c.Model().c)
+}
+
 type ModelParams struct {
 	NumGpuLayers int
 	MainGpu      int
@@ -298,6 +312,10 @@ func (m *Model) AddBOSToken() bool {
 	return bool(C.llama_vocab_get_add_bos(m.Vocab()))
 }
 
+func (m *Model) AddEOSToken() bool {
+	return bool(C.llama_add_eos_token(m.c))
+}
+
 func (m *Model) ApplyLoraFromFile(context *Context, loraPath string, scale float32, threads int) error {
 	cLoraPath := C.CString(loraPath)
 	defer C.free(unsafe.Pointer(cLoraPath))
diff --git a/runner/llamarunner/runner.go b/runner/llamarunner/runner.go
index 82880c98..6ffae7a6 100644
--- a/runner/llamarunner/runner.go
+++ b/runner/llamarunner/runner.go
@@ -823,6 +823,94 @@ func (s *Server) health(w http.ResponseWriter, r *http.Request) {
 	}
 }
 
+type RerankRequest struct {
+	Model       string   `json:"model"`
+	Query       string   `json:"query"`
+	TopN        int      `json:"top_n"`     // return top N documents
+	Documents   []string `json:"documents"` // list of documents to rerank
+	CachePrompt bool     `json:"cache_prompt"`
+}
+
+type RerankResponse struct {
+	Results []struct {
+		Index          int     `json:"index"`
+		RelevanceScore float32 `json:"relevance_score"`
+	} `json:"results"`
+}
+
+func (s *Server) rerank(w http.ResponseWriter, r *http.Request) {
+	var req RerankRequest
+	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
+		http.Error(w, fmt.Sprintf("bad rereank request: %s", err), http.StatusBadRequest)
+		return
+	}
+	w.Header().Set("Content-Type", "application/json")
+
+	var rsp RerankResponse
+	rsp.Results = make([]struct {
+		Index          int     `json:"index"`
+		RelevanceScore float32 `json:"relevance_score"`
+	}, len(req.Documents))
+
+	for i, doc := range req.Documents {
+		// reranking prompt format: [BOS]query[EOS][SEP]doc[EOS]
+		p := ""
+		if !s.model.AddBOSToken() {
+			p += s.model.TokenToPiece(int(s.lc.GetTokenBOS()))
+		}
+		p += req.Query + s.model.TokenToPiece(int(s.lc.GetTokenEOS())) + s.model.TokenToPiece(int(s.lc.GetTokenSEP())) + doc
+		if !s.model.AddEOSToken() {
+			p += s.model.TokenToPiece(int(s.lc.GetTokenEOS()))
+		}
+		seq, err := s.NewSequence(p, nil, NewSequenceParams{embedding: true})
+		if err != nil {
+			http.Error(w, fmt.Sprintf("Failed to create new sequence: %v", err), http.StatusInternalServerError)
+			return
+		}
+
+		// Ensure there is a place to put the sequence, released when removed from s.seqs
+		if err := s.seqsSem.Acquire(r.Context(), 1); err != nil {
+			if errors.Is(err, context.Canceled) {
+				slog.Info("aborting reranking request due to client closing the connection")
+			} else {
+				slog.Error("Failed to acquire semaphore", "error", err)
+			}
+			return
+		}
+
+		s.mu.Lock()
+		found := false
+		for i, sq := range s.seqs {
+			if sq == nil {
+				seq.cache, seq.inputs, err = s.cache.LoadCacheSlot(seq.inputs, req.CachePrompt)
+				if err != nil {
+					s.mu.Unlock()
+					http.Error(w, fmt.Sprintf("Failed to load cache: %v", err), http.StatusInternalServerError)
+					return
+				}
+				s.seqs[i] = seq
+				s.cond.Signal()
+				found = true
+				break
+			}
+		}
+		s.mu.Unlock()
+
+		if !found {
+			http.Error(w, "could not find an available sequence", http.StatusInternalServerError)
+			return
+		}
+
+		score := <-seq.embedding
+		rsp.Results[i].Index = i
+		rsp.Results[i].RelevanceScore = score[0]
+	}
+
+	if err := json.NewEncoder(w).Encode(&rsp); err != nil {
+		http.Error(w, fmt.Sprintf("failed to encode response: %v", err), http.StatusInternalServerError)
+	}
+}
+
 type multiLPath []string
 
 func (m *multiLPath) Set(value string) error {
@@ -844,6 +932,7 @@ func (s *Server) loadModel(
 	flashAttention bool,
 	threads int,
 	multiUserCache bool,
+	reranking bool,
 ) {
 	var err error
 	s.model, err = llama.LoadModelFromFile(mpath, params)
@@ -851,7 +940,7 @@ func (s *Server) loadModel(
 		panic(err)
 	}
 
-	ctxParams := llama.NewContextParams(kvSize, s.batchSize*s.parallel, s.parallel, threads, flashAttention, kvCacheType)
+	ctxParams := llama.NewContextParams(kvSize, s.batchSize*s.parallel, s.parallel, threads, flashAttention, kvCacheType, reranking)
 	s.lc, err = llama.NewContextWithModel(s.model, ctxParams)
 	if err != nil {
 		panic(err)
@@ -901,6 +990,7 @@ func Execute(args []string) error {
 	mlock := fs.Bool("mlock", false, "force system to keep model in RAM rather than swapping or compressing")
 	tensorSplit := fs.String("tensor-split", "", "fraction of the model to offload to each GPU, comma-separated list of proportions")
 	multiUserCache := fs.Bool("multiuser-cache", false, "optimize input cache algorithm for multiple users")
+	reranking := flag.Bool("reranking", false, "enable reranking")
 
 	var lpaths multiLPath
 	fs.Var(&lpaths, "lora", "Path to lora layer file (can be specified multiple times)")
@@ -963,7 +1053,7 @@ func Execute(args []string) error {
 	}
 
 	server.ready.Add(1)
-	go server.loadModel(params, *mpath, lpaths, *ppath, *kvSize, *kvCacheType, *flashAttention, *threads, *multiUserCache)
+	go server.loadModel(params, *mpath, lpaths, *ppath, *kvSize, *kvCacheType, *flashAttention, *threads, *multiUserCache, *reranking)
 
 	server.cond = sync.NewCond(&server.mu)
 
@@ -984,6 +1074,7 @@ func Execute(args []string) error {
 	mux.HandleFunc("/embedding", server.embeddings)
 	mux.HandleFunc("/completion", server.completion)
 	mux.HandleFunc("/health", server.health)
+	mux.HandleFunc("/rerank", server.rerank)
 
 	httpServer := http.Server{
 		Handler: mux,
-- 
2.48.1


From 217b29e8af1da4178f0feaaa9e6200005b99916f Mon Sep 17 00:00:00 2001
From: Liu Yuan <namei.unix@gmail.com>
Date: Tue, 15 Oct 2024 18:46:35 +0800
Subject: [PATCH 2/5] route: add rerank support

Signed-off-by: Liu Yuan <namei.unix@gmail.com>
---
 api/types.go     | 17 ++++++++++++
 llm/server.go    | 64 ++++++++++++++++++++++++++++++++++++++++++
 server/routes.go | 72 ++++++++++++++++++++++++++++++++++++++++++++++++
 3 files changed, 153 insertions(+)

diff --git a/api/types.go b/api/types.go
index 637ca204..561a803e 100644
--- a/api/types.go
+++ b/api/types.go
@@ -294,6 +294,23 @@ type EmbeddingResponse struct {
 	Embedding []float64 `json:"embedding"`
 }
 
+type RerankRequest struct {
+	Model     string                 `json:"model"`
+	Query     string                 `json:"query"`
+	TopN      int                    `json:"top_n"`     // return top N documents
+	Documents []string               `json:"documents"` // list of documents to rerank
+	KeepAlive *Duration              `json:"keep_alive,omitempty"`
+	Options   map[string]interface{} `json:"options,omitempty"`
+}
+
+type RerankResponse struct {
+	Model   string `json:"model"`
+	Results []struct {
+		Document       string  `json:"document"`
+		RelevanceScore float32 `json:"relevance_score"`
+	} `json:"results"`
+}
+
 // CreateRequest is the request passed to [Client.Create].
 type CreateRequest struct {
 	Model    string `json:"model"`
diff --git a/llm/server.go b/llm/server.go
index f35df295..bf0aab0f 100644
--- a/llm/server.go
+++ b/llm/server.go
@@ -37,6 +37,7 @@ type LlamaServer interface {
 	WaitUntilRunning(ctx context.Context) error
 	Completion(ctx context.Context, req CompletionRequest, fn func(CompletionResponse)) error
 	Embedding(ctx context.Context, input string) ([]float32, error)
+	Rerank(ctx context.Context, req RerankRequest, fn func(RerankResponse)) error
 	Tokenize(ctx context.Context, content string) ([]int, error)
 	Detokenize(ctx context.Context, tokens []int) (string, error)
 	Close() error
@@ -925,6 +926,69 @@ func (s *llmServer) Embedding(ctx context.Context, input string) ([]float32, err
 	return e.Embedding, nil
 }
 
+type RerankRequest struct {
+	Model     string   `json:"model"`
+	Query     string   `json:"query"`
+	TopN      int      `json:"top_n"`     // return top N documents
+	Documents []string `json:"documents"` // list of documents to rerank
+}
+type RerankResponse struct {
+	Results []struct {
+		Index          int     `json:"index"`
+		RelevanceScore float32 `json:"relevance_score"`
+	} `json:"results"`
+}
+
+func (s *llmServer) Rerank(ctx context.Context, req RerankRequest, fn func(RerankResponse)) error {
+	if err := s.sem.Acquire(ctx, 1); err != nil {
+		slog.Error("Failed to acquire semaphore", "error", err)
+		return err
+	}
+	defer s.sem.Release(1)
+
+	status, err := s.getServerStatusRetry(ctx)
+	if err != nil {
+		return err
+	} else if status != ServerStatusReady {
+		return fmt.Errorf("unexpected server status: %s", status.ToString())
+	}
+
+	data, err := json.Marshal(req)
+	if err != nil {
+		return fmt.Errorf("error marshaling rerank data: %w", err)
+	}
+
+	r, err := http.NewRequestWithContext(ctx, http.MethodPost, fmt.Sprintf("http://127.0.0.1:%d/rerank", s.port), bytes.NewBuffer(data))
+	if err != nil {
+		return fmt.Errorf("error creating rerank request: %w", err)
+	}
+	r.Header.Set("Content-Type", "application/json")
+
+	resp, err := http.DefaultClient.Do(r)
+	if err != nil {
+		return fmt.Errorf("do rerank request: %w", err)
+	}
+	defer resp.Body.Close()
+
+	body, err := io.ReadAll(resp.Body)
+	if err != nil {
+		return fmt.Errorf("error reading rerank response: %w", err)
+	}
+
+	if resp.StatusCode >= 400 {
+		log.Printf("llm rerank error: %s", body)
+		return fmt.Errorf("%s", body)
+	}
+
+	var rr RerankResponse
+	if err := json.Unmarshal(body, &rr); err != nil {
+		return fmt.Errorf("unmarshal tokenize response: %w", err)
+	}
+	fn(rr)
+
+	return nil
+}
+
 type TokenizeRequest struct {
 	Content string `json:"content"`
 }
diff --git a/server/routes.go b/server/routes.go
index b09e96ca..b5e0dbcf 100644
--- a/server/routes.go
+++ b/server/routes.go
@@ -19,6 +19,7 @@ import (
 	"os/signal"
 	"path/filepath"
 	"slices"
+	"sort"
 	"strings"
 	"syscall"
 	"time"
@@ -369,6 +370,76 @@ func (s *Server) GenerateHandler(c *gin.Context) {
 	streamResponse(c, ch)
 }
 
+func (s *Server) RerankHandler(c *gin.Context) {
+	var req api.RerankRequest
+	if err := c.ShouldBindJSON(&req); errors.Is(err, io.EOF) {
+		c.AbortWithStatusJSON(http.StatusBadRequest, gin.H{"error": "missing request body"})
+		return
+	} else if err != nil {
+		c.AbortWithStatusJSON(http.StatusBadRequest, gin.H{"error": err.Error()})
+		return
+	}
+
+	switch {
+	case len(req.Documents) == 0:
+		c.AbortWithStatusJSON(http.StatusBadRequest, gin.H{"error": "Documents cannot be empty"})
+		return
+	case req.Query == "":
+		c.AbortWithStatusJSON(http.StatusBadRequest, gin.H{"error": "Query cannot be empty"})
+		return
+	case req.TopN < 0:
+		c.AbortWithStatusJSON(http.StatusBadRequest, gin.H{"error": "TopN cannot be negative"})
+		return
+	}
+
+	r, _, _, err := s.scheduleRunner(c.Request.Context(), req.Model, []Capability{}, req.Options, req.KeepAlive)
+	if err != nil {
+		handleScheduleError(c, req.Model, err)
+		return
+	}
+
+	llmreq := llm.RerankRequest{
+		Model:     req.Model,
+		Query:     req.Query,
+		TopN:      req.TopN,
+		Documents: req.Documents,
+	}
+	err = r.Rerank(c.Request.Context(), llmreq, func(rr llm.RerankResponse) {
+		sort.SliceStable(rr.Results, func(i, j int) bool {
+			return rr.Results[i].RelevanceScore > rr.Results[j].RelevanceScore
+		})
+
+		var topn int
+		if req.TopN == 0 {
+			topn = len(rr.Results) // if TopN is unset, return all results
+		} else {
+			topn = min(len(rr.Results), req.TopN)
+		}
+		topResults := rr.Results[:topn]
+
+		rsp := api.RerankResponse{
+			Model: req.Model,
+			Results: make([]struct {
+				Document       string  `json:"document"`
+				RelevanceScore float32 `json:"relevance_score"`
+			}, topn),
+		}
+
+		for i, result := range topResults {
+			rsp.Results[i].Document = req.Documents[result.Index]
+			rsp.Results[i].RelevanceScore = result.RelevanceScore
+		}
+
+		c.JSON(http.StatusOK, rsp)
+	})
+
+	if err != nil {
+		slog.Info(fmt.Sprintf("rerank failed: %v", err))
+		c.JSON(http.StatusInternalServerError, gin.H{"error": "failed to rerank"})
+		return
+	}
+}
+
 func (s *Server) EmbedHandler(c *gin.Context) {
 	checkpointStart := time.Now()
 	var req api.EmbedRequest
@@ -1191,6 +1262,7 @@ func (s *Server) GenerateRoutes(rc *ollama.Registry) (http.Handler, error) {
 	r.POST("/api/chat", s.ChatHandler)
 	r.POST("/api/embed", s.EmbedHandler)
 	r.POST("/api/embeddings", s.EmbeddingsHandler)
+	r.POST("/api/rerank", s.RerankHandler)
 
 	// Inference (OpenAI compatibility)
 	r.POST("/v1/chat/completions", openai.ChatMiddleware(), s.ChatHandler)
-- 
2.48.1


From 3ee8e690a5148f432200459e6db3396022e3374b Mon Sep 17 00:00:00 2001
From: Liu Yuan <namei.unix@gmail.com>
Date: Thu, 31 Oct 2024 22:48:27 +0800
Subject: [PATCH 3/5] enable --reranking flag for rerank handler while starting
 server

Signed-off-by: Liu Yuan <namei.unix@gmail.com>
---
 api/types.go     | 1 +
 llm/server.go    | 4 ++++
 server/routes.go | 4 ++++
 3 files changed, 9 insertions(+)

diff --git a/api/types.go b/api/types.go
index 561a803e..89bac455 100644
--- a/api/types.go
+++ b/api/types.go
@@ -243,6 +243,7 @@ type Runner struct {
 	UseMMap   *bool `json:"use_mmap,omitempty"`
 	UseMLock  bool  `json:"use_mlock,omitempty"`
 	NumThread int   `json:"num_thread,omitempty"`
+	Reranking bool  `json:"reranking,omitempty"`
 }
 
 // EmbedRequest is the request passed to [Client.Embed].
diff --git a/llm/server.go b/llm/server.go
index bf0aab0f..75f29382 100644
--- a/llm/server.go
+++ b/llm/server.go
@@ -136,6 +136,10 @@ func NewLlamaServer(gpus discover.GpuInfoList, model string, f *ggml.GGML, adapt
 		"--batch-size", strconv.Itoa(opts.NumBatch),
 	}
 
+	if opts.Reranking {
+		params = append(params, "--reranking")
+	}
+
 	if opts.NumGPU >= 0 {
 		params = append(params, "--n-gpu-layers", strconv.Itoa(opts.NumGPU))
 	}
diff --git a/server/routes.go b/server/routes.go
index b5e0dbcf..1317799e 100644
--- a/server/routes.go
+++ b/server/routes.go
@@ -392,6 +392,10 @@ func (s *Server) RerankHandler(c *gin.Context) {
 		return
 	}
 
+	if req.Options == nil {
+		req.Options = make(map[string]any)
+	}
+	req.Options["reranking"] = true
 	r, _, _, err := s.scheduleRunner(c.Request.Context(), req.Model, []Capability{}, req.Options, req.KeepAlive)
 	if err != nil {
 		handleScheduleError(c, req.Model, err)
-- 
2.48.1


From 78bec9e9b8375926b802917e71cd64a9ffb17acf Mon Sep 17 00:00:00 2001
From: Roel Aaij <roel.aaij@nikhef.nl>
Date: Mon, 17 Feb 2025 10:45:48 +0100
Subject: [PATCH 4/5] Follow developments in how the runners are done

---
 llm/server.go                | 7 ++++++-
 runner/llamarunner/runner.go | 2 +-
 2 files changed, 7 insertions(+), 2 deletions(-)

diff --git a/llm/server.go b/llm/server.go
index 75f29382..80afdc91 100644
--- a/llm/server.go
+++ b/llm/server.go
@@ -945,11 +945,16 @@ type RerankResponse struct {
 
 func (s *llmServer) Rerank(ctx context.Context, req RerankRequest, fn func(RerankResponse)) error {
 	if err := s.sem.Acquire(ctx, 1); err != nil {
-		slog.Error("Failed to acquire semaphore", "error", err)
+		if errors.Is(err, context.Canceled) {
+			slog.Info("aborting completion request due to client closing the connection")
+		} else {
+			slog.Error("Failed to acquire semaphore", "error", err)
+		}
 		return err
 	}
 	defer s.sem.Release(1)
 
+	// Make sure the server is ready
 	status, err := s.getServerStatusRetry(ctx)
 	if err != nil {
 		return err
diff --git a/runner/llamarunner/runner.go b/runner/llamarunner/runner.go
index 6ffae7a6..8b689c7e 100644
--- a/runner/llamarunner/runner.go
+++ b/runner/llamarunner/runner.go
@@ -990,7 +990,7 @@ func Execute(args []string) error {
 	mlock := fs.Bool("mlock", false, "force system to keep model in RAM rather than swapping or compressing")
 	tensorSplit := fs.String("tensor-split", "", "fraction of the model to offload to each GPU, comma-separated list of proportions")
 	multiUserCache := fs.Bool("multiuser-cache", false, "optimize input cache algorithm for multiple users")
-	reranking := flag.Bool("reranking", false, "enable reranking")
+	reranking := fs.Bool("reranking", false, "enable reranking")
 
 	var lpaths multiLPath
 	fs.Var(&lpaths, "lora", "Path to lora layer file (can be specified multiple times)")
-- 
2.48.1


From 48bd776f2057240fbce41a1feb492177181d74b1 Mon Sep 17 00:00:00 2001
From: Roel Aaij <roel.aaij@nikhef.nl>
Date: Wed, 5 Mar 2025 13:46:36 +0100
Subject: [PATCH 5/5] Follow changes in llama.cpp

---
 llama/llama.go | 8 ++++----
 1 file changed, 4 insertions(+), 4 deletions(-)

diff --git a/llama/llama.go b/llama/llama.go
index 73b9452a..c18b42f4 100644
--- a/llama/llama.go
+++ b/llama/llama.go
@@ -217,15 +217,15 @@ func (c *Context) GetEmbeddingsIth(i int) []float32 {
 }
 
 func (c *Context) GetTokenBOS() C.llama_token {
-	return C.llama_token_bos(c.Model().c)
+	return C.llama_vocab_bos(c.Model().Vocab())
 }
 
 func (c *Context) GetTokenEOS() C.llama_token {
-	return C.llama_token_eos(c.Model().c)
+	return C.llama_vocab_eos(c.Model().Vocab())
 }
 
 func (c *Context) GetTokenSEP() C.llama_token {
-	return C.llama_token_sep(c.Model().c)
+	return C.llama_vocab_sep(c.Model().Vocab())
 }
 
 type ModelParams struct {
@@ -313,7 +313,7 @@ func (m *Model) AddBOSToken() bool {
 }
 
 func (m *Model) AddEOSToken() bool {
-	return bool(C.llama_add_eos_token(m.c))
+	return bool(C.llama_vocab_get_add_eos(m.Vocab()))
 }
 
 func (m *Model) ApplyLoraFromFile(context *Context, loraPath string, scale float32, threads int) error {
-- 
2.48.1

